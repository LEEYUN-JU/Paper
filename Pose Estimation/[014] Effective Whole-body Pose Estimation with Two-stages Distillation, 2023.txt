<Abstract>
- Knowledge Distillation기법을 사용한 pose estimation
- 사람의 center를 찾아서 이용하는 것이 pose estimation에 있어서 가장 중요한 부분임
- 데이터 한계 극복을 위해서 UBody 데이터 셋 추가로 이용함
- 첫번째 단계에서는 visible & invisible 키포인트를 supervise하는 것이 목표이다.
- 두번째 단계에서는 성능을 올리는 것을 목표로 한다.

<Introduction>
- 몸 전체의 자세를 추정하는 것보다, 얼굴 및 손을 포함한 whole-body를 추정하는 것이 훨씬 어려운데, 그 이유는 다음과 같다.
- 1) 몸 전체의 keypoints들이 hierarchical 한 구조로 되어 있기 때문이다.
- 2) 손이나, 얼굴의 경우 해상도가 좋지 않다.
- 3) 사람이 많을 수록, 추정해야하는 부분이 더 많이 폐색된다.
- 4) whole-body를 추정하기 위해서 학습 시키는 경우에 데이터 셋이 많이 부족하다.
- KD(Knowledge Distillation)은 계산량을 최소한으로 하고, 효율적인 성능 증대를 위한 방법이다.
- 이에 본 논문에서는 이의 장점을 pose estimation이라는 분야에 접목시키고자 한다.
- 동일한 모델 두개를 사용하며, 하나는 teacher 모델, 다른 하나는 student모델로 사용한다.
- 특히 이런 plug-and-play approach가 student 모델의 성능을 무려 20%나 높여준다.
- 본 논문에서 주장하는 contribution은 다음과 같다.
- 1) two-stage pose knowledge distillation method를 제안하며, 효율적인 whole-body pose estimation을 위함이다.
- 2) whole-body 데이터 제약을 없애기 위해서, 다양한 손 제스쳐와 얼굴 표정을 사용했다.
- 3) RTMPose를 baseline으로 사용했으며, DWPose는 효율적인 generation task역시 달성했다.

<Related work>
[2.1 2D Whole-body Pose Estimation]
- Openpose : body part의 여러 부분을 조합한 데이터 셋을 사용함
- MediaPipe : whole-body 랜드마크 감지를 위해서, 응용하기 쉽게 모델을 설계했다는 장점을 지님
- ZoomNet : 각기 다른 body parts의 다양성을 받아들이고, 이를 hierarchical single network로 해결하고, 처음으로 top-down method를 사용했다.
- ZoomNAS : sub-module을 연결함으로써, 성능을 높였다.
- TCFormer : vision token을 clustering 그리고 merging 하여, 다양한 위치, 사이즈 그리고 모양에 대한 강건성을 높일 수 있었다.
- RTMPose : pose estimation 에서의 key facotrs를 언급하며, real-time 모델을 설계하고, COCO-Whole body 데이터 셋에서 SOTA를 달성했다.

[2.2 Knowledge Distillation]
- KD 방법은 모델을 압축하는 하나의 방법이다.
- logit-based distillation 기반의 방식에서 feature-based distillation 방식으로 변경한다.
- 우리가 제안하는 DWPose는 pose estimation 분야에서 KD 방식을  효율적으로 적용한 첫번째 방식이다.

<Method>
- 레이블 데이터를 사용하지 않고, own logits을 사용하며, 성능을 높이고자 했다.
[The First-stage distillation]

[Feature-based distillation]

[Logit-based distillation]
- RTMPose는 SimCC를 baseline으로 하고 keypoints들을 예측한다.

[3.1.3 Weight-decay strategy for distillation]
- 이 부분은 distillation penalty를 점진적으로 줄이기 위해서 설계된 부분이다.
- 에포크가 증가할 수록 값이 적어진다. 

[3.2 The Second-stage distillation]
- 본 단계에서는 student 모델이 스스로 더 좋은 성능을 만들어 낼 수 있도록 설계하고자 했다.
- 학습하는 동안, student backbone은 고정되어 있는데, 이는 teacher 와 student가 완전히 똑같은 구조를 지니고 있기 때문이며, 여기서 해야할 일은 backbone에서 feature를 한번 뽑아내는 것만으로도 충분하다.  (?)

<Experiments>
- Dataset은 COCO를 쓰고, COCO내에서 한계적인 부분이 있기 때문에 추가로 UBody 데이터를 사용했다.
- two-stage distillation구조를 활용하여, 기존 구조에서 걸리는 시간의 1/5 만큼만 할애하여 학습이 가능하다.

[4.2 Main Results]
- 결과적으로 입력값이 다 다르거나, 해상도가 다름에도 불구하고 우리 모델은 비약적인 성능을 보여주었다.
- DWPose 의 목적은 student의 성능을 올리는 것이 관건이다.

<Analysis>
- Data effects : UBody 데이터를 추가적으로 사용함으로써, distillation method가 훨씬 좋아졌고, student 모델에서 좋은 성능을 보여주었다.

-이후로는 ablation study 를 진행하였지만, 개인적인 연구와는 크게 연관성이 없기에 자세히 살펴보지 않았음.

<새롭게 알게 된 용어 & 다시 정리하는 용어>
- from scratch : 처음부터, 바닥부터, 딥러닝 논문 내부에서 사용된 경우 남의 논문을 토대로 내가 직접 구현한 코드를 일컬음 
- plug and play approach : 즉시 시작, 꽂기만 하면 사용할 수 있는 것
- logit : 확률화되지 않은 예측 결과

<나중에 시도해 볼만한 것들 & 논문 뒷받침 참고가 될만한 내용>
- 논문의 마지막 부분에서, 좋은 포즈를 추정하는것이 생성 모델을 이용하는 경우에도 좋은 결과를 보여준다는 것을 보여줌.
- 자세를 추정의 정확도를 높여야 하는 이유에 대해서 뒷받침 가능한 내용일듯.

<참고한 블로그>

<논문 요약>
- distillation 기법을 활용하여, 모델의 사이즈는 줄이고 성능은 높임
- distillation 모델을 두 단계에 걸쳐서 사용함으로써, 학습 시간을 1/5 로 줄임
- COCO 데이터 셋에서 얻을 수 있는 정보의 한계를 극복하기 위해서 UBody라는 데이터 셋을 추가적으로 사용함.
