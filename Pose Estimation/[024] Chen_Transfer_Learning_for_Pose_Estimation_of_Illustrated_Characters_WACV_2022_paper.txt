<Abstract>
- 자세 추정은 많은 downstream image processing task에 사용된다.
- illustration에 관해서는 연구가 별로 없고, 대부분의 데이터 셋이 real dataset 을 사용한다.
- 이에 transfer learning을 통해 domain이 서로 다른 (사진과 그림) pose estimation을 가능하게 한다.
- 새로운 데이터 셋을 제시한다.
- SOTA 달성했댜

<Introduction>
- 기존에 연구되었던 illustartion을 가지고 pose estimation하는 경우에 synthetic dataest을 가지고 진행하고, Image Net을 backbone으로 사용하는 것에 지나치게 의존적이었다.
- 게다가 사용되었던 데이터 셋이 variation하지 않고, missing keypoints가 있었고, bounding box를 요구했다.
- 우리는 2D keypoint detecotor with SOTA performance 를 구성했다.
- novel illustration retrieval system을 통해 효과를 입증했다.
- SOTA pose estimator 이고, 두 개의 domain 으로부터 학습한다.
- 캐릭터 일러스트레이션 자세 추정을 위한 새로운 pose estimator를 제안한다.
- 다양한 자세를 위한 데이터 셋을 구성했다.

<Related Work>
[The Illustartion Domain]
- 캐리커쳐나 만화에 대한 시도는 있었지만, 우리는 anime/manga-style drawing에 집중하고자 한다.
- lineart cleaning, sketch extraction, 과 같은 문제들이 있다.
- 기존의 모델들은 적은 데이터 셋으로 학습하였다.
- 우리는 cleaner tag classification task, segmentation dataset, upgrade된 COCO keypoint dataset을 제안하고자 한다.

[Transfer Learning & Domain Adaptation]
- 상당수의 데이터 셋이 target에 의존하는 것보다 source에 의존한다.
- 대부분 source데이터 셋으로 train 하고 target데이터 셋에 맞추어서 fine-tuning을 진행 한다.
- source data와 target데이터 셋이 유사하다면 좋은 성능을 낼 수 있다.
- 이는 추출된 특징의 통계적 특성을 일치시키거나, 도메인간 입력을 변환하여 얻을 수 있다.

[Pose Estimation]
- naive baseline 은 Mask R-CNN임
- NMS, RMPE, OpenPose

[Pose Estimation Transfer]
- transfer learning in pose estimation은 natural images에서 synthetically-rendered data를 쓴다.
- 우리의 연구와 가장 비슷한것은 MikuMikuDance (MMD) 를 사용한 연구이지만 public dataset이 아니다.
- 이와 유사한 데이터 셋인 Anime Drawings Dataset(ADD) 를 수집했다.

<Method & Architectures>
- feature concatenation and feature matching 을 위한 pose estimator를 제안한다.

[Pose Estimation Transfer Model]
- feature concatenation, and feature matching 을 제안한다.
- we focus on combining source features to predict keypoints 
- we chose Mask R-CNN as it showed signiﬁcantly better out-of-the-box generalization to illustrations than OpenPose 
- In order to perform the concatenation, it learns shallow feature converters for each source to decrease the feature channel count and allow bilinear sampling to a common higher resolution. 
- The combined features are fed to the head, consisting of a shallow converter and two ResNet blocks.
- 최종 output은 총 25개의 heatmap(17개의 COCO, 8개의 midpoints)
- pixel-wise binary across-entropy loss on each heatmap
- we simultaneously train an additional matching network that predicts features from the expensive task-speciﬁc model using features from the domain-speciﬁc model.
- Mask R-CNN을 training하긴 했지만, 추론 과정에서 사용되지는 않았다
- 단조로운 구조임에도 불구하고 좋은 성능을 보여주었다

[ResNet Tagger]
- ResNet50 ﬁne-tuned as an illustration tagger를 backbone으로 사용했다
- tagging task는 multi-label classification과 동등하다
- Danbooru ﬁne-tuning signiﬁcantly boosts transfer performance
- 더 명시적이고 중복되지 않는 클래스를 정의하는 태그 그룹의 규칙서를 수작업으로 개발했습니다(ex: 머리에 손을 댄다, 정리한다, 넣는다 가 같은 의미로 사용되지만 tagging이 달라서 서로 다르게 classification 되어 버림 이를 해결하고자 했다는 내용)
- 덜 빈전한 클래스의 학습을 강조하도록 함

[Character Segmentation  &  Bounding  Boxes]
- DeepLabv3를 기반으로 한다
- 더 정교한 segmentation을 위해 head끝에 세 개의 레이어를 추가함
- 전체 모델 fine-tuning

<Data Collection>
- we  train  with  random  image rotation,  translation, scaling,  ﬂipping, and  recoloring
- AnimeDrawingsDataset (ADD) 사용함
- 

<새롭게 알게 된 용어 & 다시 정리하는 용어>
- 

<나중에 시도해 볼만한 것들 & 논문 뒷받침 참고가 될만한 내용>
- 

<참고한 블로그>

<논문 요약>
- feature를 추출하는 모델이 두 개 있고, 추출된 feature를 concate하게 된다
- 

